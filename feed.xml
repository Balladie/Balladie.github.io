<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://balladie.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://balladie.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-05T13:25:14+00:00</updated><id>https://balladie.github.io/feed.xml</id><title type="html">Gangin Park</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Block-Causal Distillation for Discrete Speech Generation</title><link href="https://balladie.github.io/blog/2026/bdtts/" rel="alternate" type="text/html" title="Block-Causal Distillation for Discrete Speech Generation"/><published>2026-02-05T00:00:00+00:00</published><updated>2026-02-05T00:00:00+00:00</updated><id>https://balladie.github.io/blog/2026/bdtts</id><content type="html" xml:base="https://balladie.github.io/blog/2026/bdtts/"><![CDATA[<blockquote> <p>I did a quick exploration on how autoregressive speech synthesis could leverage from <strong>block-causality</strong> applied to the latest discrete text-to-speech models. Despite their low-confidence outputs, it accelerates the inference speed up to <strong>~x2 times</strong> compared to the original in a model-native way, while getting minimal degradation on speech quality and zero-shot capability. All the existing models of similar mechanism like hybrid architecture could be accelerated in the same way, in a <strong>data-free</strong> manner that does not require real labels</p> </blockquote> <h1 id="background">Background</h1> <p>I had recently worked on building foundation text-to-speech (TTS) models from scratch in purpose of the best quality in our local language. But that is not the end: we have to make it work and run fast in production. There are multiple ways of engineering and optimizing them, but one of the natural curiosity that comes in mind was: <em>How much we can make it faster in a model-native way? Can we enable capability to generate faster on its own than it has been?</em></p> <p>Many recent TTS models have been built upon language modeling, and many of them (especially for relatively large-scale ones) are learned to generate discrete tokens for vector quantization codecs, as they provide decent reconstruction quality. To learn faster generation, the model should learn to output multiple tokens in parallel, and thatâ€™s where Block Diffusion <d-cite key="arriola2025block"></d-cite> is one of the ways that can take place.</p> <p>#</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/epicaly-short-113909.mp3" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div>]]></content><author><name></name></author><summary type="html"><![CDATA[I did a quick exploration on how autoregressive speech synthesis could leverage from block-causality applied to the latest discrete text-to-speech models. Despite their low-confidence outputs, it accelerates the inference speed up to ~x2 times compared to the original in a model-native way, while getting minimal degradation on speech quality and zero-shot capability. All the existing models of similar mechanism like hybrid architecture could be accelerated in the same way, in a data-free manner that does not require real labels]]></summary></entry></feed>