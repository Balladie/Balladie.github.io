---
layout: distill
title: Block Distillation for Discrete Speech Generation
date: 2026-02-05 00:00:00
description: I had a quick (and very straightforward) exploration on how autoregressive speech synthesis could leverage from block-causality applied to the latest discrete text-to-speech models. Despite their low-confidence outputs, it accelerates the inference speed up to <strong>~x2 times</strong> compared to the original in a model-native way, while getting minimal degradation on speech quality and zero-shot capability. All the existing models of similar mechanism like hybrid architecture could be accelerated in the same way, in a <strong>data-free</strong> manner that does not require real labels.
tags:
categories:
bibliography: 2026-02-05-bdtts.bib
# toc:
#   - name: Results
#   - name: Background
#   - name: Methods
#   - name: Future Works
---
> 


## Results

(being updated)


## Background

I had recently worked on building foundation text-to-speech (TTS) models from scratch in purpose of the best quality in our local language. But that is not the end: we have to make it work and run fast in production. There are multiple ways of engineering and optimizing them, but one of the natural curiosity that comes in mind was: *How much we can make it faster in a model-native way? Can we enable capability to generate faster on its own than it has been?*

Many recent TTS models have been built upon language modeling, and many of them (especially for relatively large-scale ones) are learned to generate discrete tokens for vector quantization codecs, as they provide decent reconstruction quality. To learn faster generation, the model should learn to output multiple tokens in parallel, and that's where masked diffusion framework could be one of the ways that takes place. 


## Methods

The training recipe is initialized and heavily borrowed from Block Diffusion<d-cite key="arriola2025block"></d-cite>, A2D-VL<d-cite key="arriola2025ar2d"></d-cite>, and Fast-dLLM<d-cite key="wu2025fast"></d-cite><d-cite key="wu2025fast2"></d-cite>: refer to them for details. Lately there are so many related works that improves further or scales up, and here I only leave some key points that would matter:

- <strong>Block Diffusion</strong>. Instead of doing full bidirectional attention for masked diffusion, Block Diffusion trains on block-causal mask to generate in a semi-autoregressive manner. Within each block the tokens are generated bidirectionally, while blocks are completed autoregressively. KV caching and parallel decoding further enables efficient inference. 

- <strong>AR initialization & difficulty annealing</strong>. To leverage from existing autoregressive (AR) models that performs well, many recent works have initialized from them to train masked diffusion language models. Choosing appropriate strategy to anneal from causal to block-causal generation during training is crucial, such as block size, position of masking, and timestep.

- <strong>Complementary mask</strong>. All token positions are included in both visible and masked states, to reduce stochasticity during training.

- <strong>Confidence-based Parallel Decoding</strong>. At inference, tokens that have high probability are decoded in parallel, enabling variable number of multiple tokens unmasked to improve efficiency.

### Problems

Unlike typical large language models that has relatively high confidence on their logits, speech generation has much more possible choices even if it's conditioned: for example, one text sequence can be matched with so many speech sequences. Given this, I noticed two practical problems that limit the extension on speech synthesis:

1. Fine-tuning on real datasets highly limits and degrades the output quality, even with the data included in the training set for AR.

2. Confidence-based decoding rarely does parallel decoding during inference.

### Data-free Block Distillation

Supervising with real data degrades quality too much, and it also requires speech tokens encoded offline from real audio data. Instead of this, we can intuitively combine AR-to-block annealed training with knowledge distillation (KD) to preserve and fully leverage from autoregressive teacher's capability. The typical objective for KD can be denoted as

$$
\mathcal{L}_{KD} = \mathop{\mathbb{E}}_{(x, y) \sim (X, Y)}[\mathcal{D}(p_T || p_S^\theta)(y|x)],
$$

where $(X, Y)$ is the input-output pairs from supervising data, $p_T$ and $p_S$ are token-level distribution from teacher and student, and $\mathcal{D}$ is a divergence. $\mathcal{D}$ is often chosen as KL divergence, and to bound the values in the safe range, it is practical to use generalized Jenson-Shannon divergence defined as

$$
\mathcal{D}_{JSD}(P || Q) = \beta \mathcal{D}_{KL}(P || \beta P + (1-\beta) Q) + (1-\beta)D_{KL}(Q ||  \beta P + (1-\beta) Q)
$$

where $\beta$ is a hyperparameter that balances the forward and reverse KL, each of them known with the behavior of mean and mode-covering.

When $Y$ is extracted from teacher, the student can be supervised with teacher predictions during distillation, making it data-free that only requires text input condition set. $Y$ can also be multiple labels such as top-k of teacher outputs which can benefit from learning diverse paths (but not to many). One can also utilize student prediction to do self-distillation, but mixing it did not help much in my case as there's at least some degradation when block-causal mask is applied.

In many cases full fine-tuning is not necessary and LoRA with small rank was sufficient in our case during distillation.

### Low-Confidence Thresholding

During inference, common threshold values for confidence-based parallel decoding is not much beneficial for speech synthesis, as they often have logits of low confidence. This is why greedy decoding in TTS results in much more weird outputs compared to text generation, and temperature value such as 1.0 is a common choice to enable minimal stochasticity required in practice. I have tuned the threshold carefully, and found that values between 0.01~0.02 gives a good trade-off point between speed and quality, which is very small compared to the one in dLLMs that often use range of 0.7~0.9. 


## Future Works

One of the limitations is that the threshold for parallel decoding is too sensitive in some cases: if it's too high it slows down, if it's too low it starts producing low-quality speech or pauses more frequently. Sampling method that is more robust to the cases of low confidence could possibly improve performance.

It could also be challenged to more difficult scenarios such as low-step distillation. There are some recent works that explores in this perspective, so it can be good exploration on finding whether it would be beneficial in practice to further reduce its NFE.

Another exploration would be more on the low-level optimization, such as leveraging from flexible-mask flash attention and more aggressive caching, pushing both the baseline and modified one to their best-optimized states, which would show true comparison on upper bound when applied in practice. 



<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.liquid path="assets/audio/epicaly-short-113909.mp3" controls=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.liquid path="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls=true %}
    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all.
</div> -->
