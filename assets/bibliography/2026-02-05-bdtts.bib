@article{arriola2025block,
  title={Block diffusion: Interpolating between autoregressive and diffusion language models},
  author={Arriola, Marianne and Gokaslan, Aaron and Chiu, Justin T and Yang, Zhihan and Qi, Zhixuan and Han, Jiaqi and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2503.09573},
  year={2025}
}

@misc{arriola2025ar2d,
    title={Adapting Autoregressive Vision Language Models for Parallel Diffusion Decoding},
    author={Arriola, Marianne and Venkat, Naveen and Granskog, Jonathan and Germanidis, Anastasis},
    year={2025},
    url={https://runwayml.com/research/autoregressive-to-diffusion-vlms},
}

@article{wu2025fast,
  title={Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding},
  author={Wu, Chengyue and Zhang, Hao and Xue, Shuchen and Liu, Zhijian and Diao, Shizhe and Zhu, Ligeng and Luo, Ping and Han, Song and Xie, Enze},
  journal={arXiv preprint arXiv:2505.22618},
  year={2025}
}

@article{wu2025fast2,
  title={Fast-dllm v2: Efficient block-diffusion llm},
  author={Wu, Chengyue and Zhang, Hao and Xue, Shuchen and Diao, Shizhe and Fu, Yonggan and Liu, Zhijian and Molchanov, Pavlo and Luo, Ping and Han, Song and Xie, Enze},
  journal={arXiv preprint arXiv:2509.26328},
  year={2025}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}